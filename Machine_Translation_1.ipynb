{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvmd/2eLYetsHdR1kwo8+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardcdy/deep-learning-notebooks/blob/main/Machine_Translation_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oECc9ffIdYTB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "kqYhk9Lu5wzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "nJTCs2K5lTd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "gGAEQapwl1VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "8V1UNQDy7Ijv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple feedforward model"
      ],
      "metadata": {
        "id": "Bu_Lq4XdJCnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FF(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_vocab_size: int, target_vocab_size: int, max_seq_len: int = 30):\n",
        "    super().__init__()\n",
        "    self.lookup = nn.Embedding(input_vocab_size, 10)\n",
        "    self.fc1 = nn.Linear(max_seq_len * 10, max_seq_len * 20)\n",
        "    self.fc2 = nn.Linear(max_seq_len * 20, max_seq_len * 20)\n",
        "    self.fc3 = nn.Linear(max_seq_len * 20, max_seq_len * target_vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, max_seq_len = x.shape\n",
        "    embedded = self.lookup(x)\n",
        "    x = embedded.reshape(batch_size, max_seq_len * 10)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x).reshape((batch_size, max_seq_len, -1))\n",
        "    return F.log_softmax(x, dim=2)\n"
      ],
      "metadata": {
        "id": "Hu0klv387OBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_vocab = len(vocab_transform[SRC_LANGUAGE])\n",
        "out_vocab = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "net = FF(in_vocab, out_vocab)"
      ],
      "metadata": {
        "id": "i595vAhoA0qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def train(net: nn.Module, epochs: int = 20):\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
        "  loss = torch.nn.NLLLoss()\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    for x,y in train_dataloader:\n",
        "      net.zero_grad()\n",
        "\n",
        "      x, y = scale_transpose_seqs(x, y)\n",
        "\n",
        "      out = net(x)\n",
        "\n",
        "      l = loss(out.swapaxes(1,2), y)\n",
        "      l.backward()\n",
        "      opt.step()\n",
        "\n",
        "\n",
        "def scale_transpose_seqs(x: torch.Tensor, y: torch.Tensor, max_length: int = 30):\n",
        "  if x.shape[0] < max_length:\n",
        "    add = torch.full((max_length - x.shape[0], x.shape[1]), PAD_IDX)\n",
        "    x = torch.cat((x, add), dim=0)\n",
        "  elif x.shape[0] > max_length:\n",
        "    x = x[:max_length, :]\n",
        "\n",
        "  if y.shape[0] < max_length:\n",
        "    add = torch.full((max_length - y.shape[0], y.shape[1]), PAD_IDX)\n",
        "    y = torch.cat((y, add), dim=0)\n",
        "  elif y.shape[0] > max_length:\n",
        "    y = y[:max_length, :]\n",
        "\n",
        "  return x.to(device).long().T, y.to(device).long().T"
      ],
      "metadata": {
        "id": "Bs93Mxq0CNJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "valid_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "valid_dataloader = DataLoader(to_map_style_dataset(valid_iter), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "def example_translations(net: nn.Module, n_samples: int): \n",
        "  net.eval()\n",
        "\n",
        "  english = vocab_transform[TGT_LANGUAGE].get_itos()\n",
        "  german = vocab_transform[SRC_LANGUAGE].get_itos()\n",
        "\n",
        "  count = 0\n",
        "  for x,y in valid_dataloader:\n",
        "    print(f'Source sentence: {\" \".join(map(lambda a: german[a], x))}')\n",
        "\n",
        "    x, _ = scale_transpose_seqs(x,y)\n",
        "    out = net(x)\n",
        "\n",
        "    print(f'Translated sentence: {\" \".join(map(lambda a: english[a.argmax()], out.squeeze()))}')\n",
        "\n",
        "    count += 1\n",
        "    if count == n_samples:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "ENdBedjrFutr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_translations(net, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGwM3fy9KE5O",
        "outputId": "b8a4806d-fcec-4119-fbc7-015e632f514e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source sentence: <bos> Zwei Menschen rennen auf dem Gipfel eines Berges . <eos>\n",
            "Translated sentence: mounted natured popsicles learns Muzzled Half armor Jacket Worker lioness Ten mowing Inn Medieval wits tricks folk 94 Tournament 145 presume marching peoples effort awnings Room terrorizes toad returning resourceful\n"
          ]
        }
      ]
    }
  ]
}